{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.3 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 20.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp39-cp39-macosx_12_0_arm64.whl (239.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 239.4 MB 45.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras\n",
      "  Downloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 21.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 69.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[K     |████████████████████████████████| 508 kB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[K     |████████████████████████████████| 346 kB 33.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/farhadali/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp39-cp39-macosx_12_0_arm64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 13.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.8 MB 61.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ml-dtypes<0.5.0,>=0.4.0\n",
      "  Downloading ml_dtypes-0.4.1-cp39-cp39-macosx_10_9_universal2.whl (396 kB)\n",
      "\u001b[K     |████████████████████████████████| 396 kB 46.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 972 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.70.0-cp39-cp39-macosx_10_14_universal2.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 81.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.17.2-cp39-cp39-macosx_11_0_arm64.whl (38 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 31.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "\u001b[K     |████████████████████████████████| 417 kB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: packaging in /Users/farhadali/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.2)\n",
      "Collecting tensorboard<2.19,>=2.18\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5 MB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /Users/farhadali/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting flatbuffers>=24.3.25\n",
      "  Downloading flatbuffers-25.1.24-py2.py3-none-any.whl (30 kB)\n",
      "Collecting h5py>=3.11.0\n",
      "  Downloading h5py-3.12.1-cp39-cp39-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 72.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rich\n",
      "  Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[K     |████████████████████████████████| 242 kB 74.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting optree\n",
      "  Downloading optree-0.14.0-cp39-cp39-macosx_11_0_arm64.whl (324 kB)\n",
      "\u001b[K     |████████████████████████████████| 324 kB 18.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting namex\n",
      "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.3 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 69.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 62.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 14.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 34.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.1-cp39-cp39-macosx_10_9_universal2.whl (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "\u001b[K     |████████████████████████████████| 164 kB 61.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 35.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 22.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[K     |████████████████████████████████| 224 kB 21.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.4 in /Users/farhadali/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow) (8.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/farhadali/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow) (3.21.0)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/farhadali/Library/Python/3.9/lib/python/site-packages (from rich->keras) (2.19.1)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 26.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: mdurl, numpy, MarkupSafe, markdown-it-py, werkzeug, urllib3, tensorboard-data-server, rich, protobuf, optree, namex, ml-dtypes, markdown, idna, h5py, grpcio, charset-normalizer, certifi, absl-py, wrapt, tzdata, tqdm, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorboard, scipy, requests, regex, pytz, opt-einsum, libclang, keras, joblib, google-pasta, gast, flatbuffers, click, astunparse, tensorflow, scikit-learn, pandas, nltk\n",
      "Successfully installed MarkupSafe-3.0.2 absl-py-2.1.0 astunparse-1.6.3 certifi-2024.12.14 charset-normalizer-3.4.1 click-8.1.8 flatbuffers-25.1.24 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 h5py-3.12.1 idna-3.10 joblib-1.4.2 keras-3.8.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 nltk-3.9.1 numpy-2.0.2 opt-einsum-3.4.0 optree-0.14.0 pandas-2.2.3 protobuf-5.29.3 pytz-2024.2 regex-2024.11.6 requests-2.32.3 rich-13.9.4 scikit-learn-1.6.1 scipy-1.13.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 threadpoolctl-3.5.0 tqdm-4.67.1 tzdata-2025.1 urllib3-2.3.0 werkzeug-3.1.3 wrapt-1.17.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas tensorflow keras scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farhadali/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /Users/farhadali/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/farhadali/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/farhadali/Documents/Python/ML_Projects/TextGeneration/input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/farhadali/Documents/Python/ML_Projects/TextGeneration/input/label_texts.txt\n",
      "/Users/farhadali/Documents/Python/ML_Projects/TextGeneration/input/input_texts.txt\n",
      "/Users/farhadali/Documents/Python/ML_Projects/TextGeneration/input/dialogs.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/Users/farhadali/Documents/Python/ML_Projects/TextGeneration/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 349ms/step - accuracy: 0.7002 - loss: 2.7541\n",
      "Epoch 2/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 331ms/step - accuracy: 0.8112 - loss: 1.2139\n",
      "Epoch 3/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 337ms/step - accuracy: 0.8291 - loss: 1.1049\n",
      "Epoch 4/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 341ms/step - accuracy: 0.8359 - loss: 1.0640\n",
      "Epoch 5/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 351ms/step - accuracy: 0.8370 - loss: 1.0418\n",
      "Epoch 6/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 349ms/step - accuracy: 0.8352 - loss: 1.0373\n",
      "Epoch 7/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 354ms/step - accuracy: 0.8372 - loss: 1.0069\n",
      "Epoch 8/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 354ms/step - accuracy: 0.8364 - loss: 0.9821\n",
      "Epoch 9/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 359ms/step - accuracy: 0.8426 - loss: 0.9329\n",
      "Epoch 10/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 363ms/step - accuracy: 0.8423 - loss: 0.9151\n",
      "Epoch 11/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 369ms/step - accuracy: 0.8421 - loss: 0.9001\n",
      "Epoch 12/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 368ms/step - accuracy: 0.8439 - loss: 0.8753\n",
      "Epoch 13/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 370ms/step - accuracy: 0.8469 - loss: 0.8404\n",
      "Epoch 14/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 370ms/step - accuracy: 0.8458 - loss: 0.8326\n",
      "Epoch 15/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 373ms/step - accuracy: 0.8462 - loss: 0.8143\n",
      "Epoch 16/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 370ms/step - accuracy: 0.8476 - loss: 0.7947\n",
      "Epoch 17/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 377ms/step - accuracy: 0.8513 - loss: 0.7621\n",
      "Epoch 18/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 376ms/step - accuracy: 0.8529 - loss: 0.7406\n",
      "Epoch 19/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 381ms/step - accuracy: 0.8537 - loss: 0.7245\n",
      "Epoch 20/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 378ms/step - accuracy: 0.8542 - loss: 0.7137\n",
      "Epoch 21/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 376ms/step - accuracy: 0.8578 - loss: 0.6796\n",
      "Epoch 22/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 377ms/step - accuracy: 0.8595 - loss: 0.6689\n",
      "Epoch 23/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 378ms/step - accuracy: 0.8606 - loss: 0.6512\n",
      "Epoch 24/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 386ms/step - accuracy: 0.8643 - loss: 0.6301\n",
      "Epoch 25/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 398ms/step - accuracy: 0.8667 - loss: 0.6107\n",
      "Epoch 26/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 402ms/step - accuracy: 0.8687 - loss: 0.5959\n",
      "Epoch 27/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 403ms/step - accuracy: 0.8711 - loss: 0.5765\n",
      "Epoch 28/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 399ms/step - accuracy: 0.8745 - loss: 0.5596\n",
      "Epoch 29/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 399ms/step - accuracy: 0.8761 - loss: 0.5450\n",
      "Epoch 30/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 394ms/step - accuracy: 0.8779 - loss: 0.5331\n",
      "Epoch 31/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 395ms/step - accuracy: 0.8804 - loss: 0.5188\n",
      "Epoch 32/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 398ms/step - accuracy: 0.8835 - loss: 0.5004\n",
      "Epoch 33/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 424ms/step - accuracy: 0.8858 - loss: 0.4889\n",
      "Epoch 34/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 569ms/step - accuracy: 0.8884 - loss: 0.4708\n",
      "Epoch 35/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 362ms/step - accuracy: 0.8917 - loss: 0.4587\n",
      "Epoch 36/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 381ms/step - accuracy: 0.8937 - loss: 0.4439\n",
      "Epoch 37/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 385ms/step - accuracy: 0.8970 - loss: 0.4297\n",
      "Epoch 38/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 412ms/step - accuracy: 0.8985 - loss: 0.4180\n",
      "Epoch 39/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 443ms/step - accuracy: 0.8991 - loss: 0.4122\n",
      "Epoch 40/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 422ms/step - accuracy: 0.9013 - loss: 0.4027\n",
      "Epoch 41/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 417ms/step - accuracy: 0.9042 - loss: 0.3886\n",
      "Epoch 42/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 411ms/step - accuracy: 0.9061 - loss: 0.3788\n",
      "Epoch 43/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 404ms/step - accuracy: 0.9078 - loss: 0.3721\n",
      "Epoch 44/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 1s/step - accuracy: 0.9109 - loss: 0.3558\n",
      "Epoch 45/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 341ms/step - accuracy: 0.9130 - loss: 0.3482\n",
      "Epoch 46/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 351ms/step - accuracy: 0.9144 - loss: 0.3401\n",
      "Epoch 47/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 361ms/step - accuracy: 0.9151 - loss: 0.3382\n",
      "Epoch 48/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 376ms/step - accuracy: 0.9178 - loss: 0.3267\n",
      "Epoch 49/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 377ms/step - accuracy: 0.9196 - loss: 0.3196\n",
      "Epoch 50/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 2s/step - accuracy: 0.9222 - loss: 0.3107\n",
      "Epoch 51/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 332ms/step - accuracy: 0.9226 - loss: 0.3055\n",
      "Epoch 52/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 348ms/step - accuracy: 0.9248 - loss: 0.2987\n",
      "Epoch 53/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 356ms/step - accuracy: 0.9250 - loss: 0.2951\n",
      "Epoch 54/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 377ms/step - accuracy: 0.9271 - loss: 0.2865\n",
      "Epoch 55/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 374ms/step - accuracy: 0.9279 - loss: 0.2822\n",
      "Epoch 56/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 757ms/step - accuracy: 0.9277 - loss: 0.2825\n",
      "Epoch 57/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 4s/step - accuracy: 0.9296 - loss: 0.2749\n",
      "Epoch 58/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 334ms/step - accuracy: 0.9304 - loss: 0.2708\n",
      "Epoch 59/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 345ms/step - accuracy: 0.9315 - loss: 0.2680\n",
      "Epoch 60/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 361ms/step - accuracy: 0.9351 - loss: 0.2557\n",
      "Epoch 61/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 364ms/step - accuracy: 0.9348 - loss: 0.2569\n",
      "Epoch 62/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 369ms/step - accuracy: 0.9357 - loss: 0.2516\n",
      "Epoch 63/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 1s/step - accuracy: 0.9372 - loss: 0.2477\n",
      "Epoch 64/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 347ms/step - accuracy: 0.9370 - loss: 0.2477\n",
      "Epoch 65/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 359ms/step - accuracy: 0.9387 - loss: 0.2398\n",
      "Epoch 66/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 372ms/step - accuracy: 0.9393 - loss: 0.2387\n",
      "Epoch 67/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 373ms/step - accuracy: 0.9399 - loss: 0.2341\n",
      "Epoch 68/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 377ms/step - accuracy: 0.9404 - loss: 0.2298\n",
      "Epoch 69/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 910ms/step - accuracy: 0.9417 - loss: 0.2265\n",
      "Epoch 70/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 349ms/step - accuracy: 0.9416 - loss: 0.2256\n",
      "Epoch 71/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 361ms/step - accuracy: 0.9419 - loss: 0.2246\n",
      "Epoch 72/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 380ms/step - accuracy: 0.9421 - loss: 0.2233\n",
      "Epoch 73/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 377ms/step - accuracy: 0.9440 - loss: 0.2178\n",
      "Epoch 74/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 383ms/step - accuracy: 0.9429 - loss: 0.2198\n",
      "Epoch 75/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 407ms/step - accuracy: 0.9453 - loss: 0.2109\n",
      "Epoch 76/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 426ms/step - accuracy: 0.9454 - loss: 0.2122\n",
      "Epoch 77/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 430ms/step - accuracy: 0.9448 - loss: 0.2130\n",
      "Epoch 78/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 426ms/step - accuracy: 0.9473 - loss: 0.2055\n",
      "Epoch 79/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 426ms/step - accuracy: 0.9471 - loss: 0.2045\n",
      "Epoch 80/80\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 915ms/step - accuracy: 0.9466 - loss: 0.2049\n",
      "Input: hi, how are you doing?\n",
      "Response: bye\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    TextVectorization,\n",
    "    Embedding,\n",
    "    Dense,\n",
    "    LayerNormalization,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 1. CUSTOM PREPROCESSING\n",
    "def custom_standardization(text):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, r'[^a-z0-9\\s\\[\\]]', '')\n",
    "    return text\n",
    "\n",
    "# 2. LOAD AND PREPARE DATA\n",
    "def load_and_prepare_data(input_file, label_file, max_samples=25000, max_length=40):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        input_texts = f.readlines()\n",
    "    with open(label_file, 'r', encoding='utf-8') as f:\n",
    "        label_texts = f.readlines()\n",
    "\n",
    "    input_texts = [f\"[sos] {line.strip()} [eos]\" for line in input_texts[:max_samples]]\n",
    "    label_texts = [f\"[sos] {line.strip()} [eos]\" for line in label_texts[:max_samples]]\n",
    "    return input_texts, label_texts\n",
    "\n",
    "# 3. CREATE TEXT VECTORIZER\n",
    "def create_text_vectorizer(texts, max_tokens=10000, max_len=40):\n",
    "    vectorizer = TextVectorization(\n",
    "        max_tokens=max_tokens,\n",
    "        output_sequence_length=max_len,\n",
    "        standardize=custom_standardization\n",
    "    )\n",
    "    vectorizer.adapt(texts)\n",
    "    return vectorizer\n",
    "\n",
    "# 4. DEFINE CUSTOM LAYERS\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(max_len, d_model)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncoding, self).get_config()\n",
    "        config.update({\n",
    "            \"max_len\": self.pos_encoding.shape[1],\n",
    "            \"d_model\": self.pos_encoding.shape[2],\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000.0, (2 * (i//2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, max_len, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(max_len, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model,\n",
    "        )\n",
    "\n",
    "        # Apply sin to even indices in the array; 2i\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # Apply cos to odd indices in the array; 2i+1\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "class MultiHeadAttentionLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = Dense(d_model)\n",
    "        self.wk = Dense(d_model)\n",
    "        self.wv = Dense(d_model)\n",
    "\n",
    "        self.dense = Dense(d_model)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MultiHeadAttentionLayer, self).get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"num_heads\": self.num_heads,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0,2,1,3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
    "        \"\"\"Calculate the attention weights.\"\"\"\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttentionLayer(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(EncoderLayer, self).get_config()\n",
    "        config.update({\n",
    "            \"mha\": self.mha,\n",
    "            \"ffn\": self.ffn,\n",
    "            \"layernorm1\": self.layernorm1,\n",
    "            \"layernorm2\": self.layernorm2,\n",
    "            \"dropout1\": self.dropout1,\n",
    "            \"dropout2\": self.dropout2,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x, mask, training=False):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha1 = MultiHeadAttentionLayer(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttentionLayer(d_model, num_heads)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        self.dropout3 = Dropout(dropout_rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DecoderLayer, self).get_config()\n",
    "        config.update({\n",
    "            \"mha1\": self.mha1,\n",
    "            \"mha2\": self.mha2,\n",
    "            \"ffn\": self.ffn,\n",
    "            \"layernorm1\": self.layernorm1,\n",
    "            \"layernorm2\": self.layernorm2,\n",
    "            \"layernorm3\": self.layernorm3,\n",
    "            \"dropout1\": self.dropout1,\n",
    "            \"dropout2\": self.dropout2,\n",
    "            \"dropout3\": self.dropout3,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask, training=False):\n",
    "        # Masked MHA\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        # MHA with encoder output\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        # FFN\n",
    "        ffn_output = self.ffn(out2) # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "# 5. DEFINE MASKING FUNCTIONS\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "# 6. BUILD TRANSFORMER MODEL\n",
    "class Transformer(Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_dim, input_vocab_size, target_vocab_size, max_len, dropout_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder_embedding = Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(max_len, d_model)\n",
    "\n",
    "        self.encoder_layers = [\n",
    "            EncoderLayer(d_model, num_heads, ff_dim, dropout_rate) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.decoder_layers = [\n",
    "            DecoderLayer(d_model, num_heads, ff_dim, dropout_rate) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "        self.final_layer = Dense(target_vocab_size, activation='softmax')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Transformer, self).get_config()\n",
    "        config.update({\n",
    "            \"encoder_embedding\": self.encoder_embedding,\n",
    "            \"decoder_embedding\": self.decoder_embedding,\n",
    "            \"pos_encoding\": self.pos_encoding,\n",
    "            \"encoder_layers\": self.encoder_layers,\n",
    "            \"decoder_layers\": self.decoder_layers,\n",
    "            \"dropout\": self.dropout,\n",
    "            \"final_layer\": self.final_layer,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_seq, target_seq = inputs  # Unpack the inputs\n",
    "\n",
    "        # Create masks inside the call method\n",
    "        enc_padding_mask = create_padding_mask(input_seq)\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(target_seq)[1])\n",
    "        dec_padding_mask = create_padding_mask(input_seq)\n",
    "\n",
    "        # Encoder\n",
    "        enc_emb = self.encoder_embedding(input_seq)  # (batch_size, input_seq_len, d_model)\n",
    "        enc_emb = self.pos_encoding(enc_emb)\n",
    "        enc_emb = self.dropout(enc_emb, training=training)\n",
    "\n",
    "        enc_output = enc_emb\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, enc_padding_mask, training=training)\n",
    "\n",
    "        # Decoder\n",
    "        dec_emb = self.decoder_embedding(target_seq)  # (batch_size, target_seq_len, d_model)\n",
    "        dec_emb = self.pos_encoding(dec_emb)\n",
    "        dec_emb = self.dropout(dec_emb, training=training)\n",
    "\n",
    "        dec_output = dec_emb\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output, _, _ = dec_layer(dec_output, enc_output, look_ahead_mask, dec_padding_mask, training=training)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, target_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "# 7. INFERENCE FUNCTION\n",
    "def decode_sequence(model, input_vectorizer, label_vectorizer, input_text, max_len=40):\n",
    "    vocab = label_vectorizer.get_vocabulary()\n",
    "    start_token = vocab.index('[sos]')\n",
    "    end_token = vocab.index('[eos]')\n",
    "\n",
    "    input_seq = input_vectorizer([f\"[sos] {input_text.strip()} [eos]\"])\n",
    "    input_seq = tf.cast(input_seq, tf.int32)  # Ensure dtype matches\n",
    "\n",
    "    output_seq = tf.expand_dims([start_token], 0)  # (1, 1)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        predictions = model(inputs=[input_seq, output_seq], training=False)  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        # Select the last word\n",
    "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)  # (batch_size, 1)\n",
    "\n",
    "        if predicted_id.numpy()[0][0] == end_token:\n",
    "            break\n",
    "\n",
    "        # Ensure predicted_id is int32 to match output_seq\n",
    "        predicted_id = tf.cast(predicted_id, output_seq.dtype)\n",
    "\n",
    "        # Concatenate the predicted_id to the output_seq\n",
    "        output_seq = tf.concat([output_seq, predicted_id], axis=-1)\n",
    "\n",
    "    predicted_sentence = [\n",
    "        vocab[token_id] for token_id in output_seq.numpy()[0] if token_id < len(vocab)\n",
    "    ]\n",
    "    return ' '.join(predicted_sentence).replace('[sos]', '').replace('[eos]', '').strip()\n",
    "\n",
    "# 8. TRAINING AND PREDICTION\n",
    "# Paths\n",
    "input_file = '/Users/farhadali/Documents/Python/ML_Projects/TextGeneration/input/input_texts.txt'\n",
    "label_file = '/Users/farhadali/Documents/Python/ML_Projects/TextGeneration/input/label_texts.txt'\n",
    "\n",
    "# Parameters\n",
    "max_samples = 25000\n",
    "max_length = 40\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "num_layers = 2\n",
    "dropout_rate = 0.1\n",
    "epochs = 80\n",
    "batch_size = 64\n",
    "\n",
    "# Load and prepare data\n",
    "input_texts, label_texts = load_and_prepare_data(input_file, label_file, max_samples, max_length)\n",
    "\n",
    "# Create vectorizers\n",
    "input_vectorizer = create_text_vectorizer(input_texts, max_tokens=10000, max_len=max_length)\n",
    "label_vectorizer = create_text_vectorizer(label_texts, max_tokens=10000, max_len=max_length)\n",
    "\n",
    "# Vectorize data\n",
    "input_data = input_vectorizer(input_texts)\n",
    "input_data = tf.cast(input_data, tf.int32)  # Cast to int32\n",
    "\n",
    "label_data = label_vectorizer(label_texts)\n",
    "label_data = tf.cast(label_data, tf.int32)  # Cast to int32\n",
    "\n",
    "# Prepare decoder input and target\n",
    "label_input_data = label_data[:, :-1]\n",
    "label_target_data = label_data[:, 1:]\n",
    "\n",
    "# Vocabulary sizes\n",
    "input_vocab_size = len(input_vectorizer.get_vocabulary())\n",
    "target_vocab_size = len(label_vectorizer.get_vocabulary())\n",
    "\n",
    "# Build model\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    max_len=max_length,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "transformer.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "transformer.fit(\n",
    "    x=[input_data, label_input_data],\n",
    "    y=label_target_data,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Test inference\n",
    "test_input = \"hi, how are you doing?\"\n",
    "response = decode_sequence(transformer, input_vectorizer, label_vectorizer, test_input)\n",
    "print(\"Input:\", test_input)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: i've been great. what about you?\n",
      "Response: i like all my classmates too\n"
     ]
    }
   ],
   "source": [
    "# Test inference\n",
    "test_input = \"i've been great. what about you?\"\n",
    "response = decode_sequence(transformer, input_vectorizer, label_vectorizer, test_input)\n",
    "print(\"Input:\", test_input)\n",
    "print(\"Response:\", response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tk\n",
      "  Downloading tk-0.1.0-py3-none-any.whl (3.9 kB)\n",
      "Installing collected packages: tk\n",
      "Successfully installed tk-0.1.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 13:11:36.110 Python[11662:1847036] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-01-29 13:11:36.110 Python[11662:1847036] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farhadali/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "\n",
    "# Function to handle the button click event\n",
    "def generate_response():\n",
    "    input_text = input_textbox.get(\"1.0\", tk.END).strip()\n",
    "    if input_text:\n",
    "        response = decode_sequence(transformer, input_vectorizer, label_vectorizer, input_text)\n",
    "        output_textbox.config(state=tk.NORMAL)\n",
    "        output_textbox.delete(\"1.0\", tk.END)\n",
    "        output_textbox.insert(tk.END, response)\n",
    "        output_textbox.config(state=tk.DISABLED)\n",
    "    else:\n",
    "        output_textbox.config(state=tk.NORMAL)\n",
    "        output_textbox.delete(\"1.0\", tk.END)\n",
    "        output_textbox.insert(tk.END, \"Please enter some text to generate a response.\")\n",
    "        output_textbox.config(state=tk.DISABLED)\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Text Generation with Transformer\")\n",
    "\n",
    "# Create a frame for the input\n",
    "input_frame = tk.Frame(root)\n",
    "input_frame.pack(padx=10, pady=10)\n",
    "\n",
    "# Create a label for the input textbox\n",
    "input_label = tk.Label(input_frame, text=\"Enter your text:\")\n",
    "input_label.pack(anchor=tk.W)\n",
    "\n",
    "# Create a textbox for input\n",
    "input_textbox = tk.Text(input_frame, height=5, width=50)\n",
    "input_textbox.pack(padx=10, pady=10)\n",
    "\n",
    "# Create a frame for the output\n",
    "output_frame = tk.Frame(root)\n",
    "output_frame.pack(padx=10, pady=10)\n",
    "\n",
    "# Create a label for the output textbox\n",
    "output_label = tk.Label(output_frame, text=\"Generated Response:\")\n",
    "output_label.pack(anchor=tk.W)\n",
    "\n",
    "# Create a scrolled textbox for output\n",
    "output_textbox = scrolledtext.ScrolledText(output_frame, height=5, width=50, state=tk.DISABLED)\n",
    "output_textbox.pack(padx=10, pady=10)\n",
    "\n",
    "# Create a button to generate the response\n",
    "generate_button = tk.Button(root, text=\"Generate Response\", command=generate_response)\n",
    "generate_button.pack(pady=10)\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformer' is not defined"
     ]
    }
   ],
   "source": [
    "transformer.save('transformer_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
